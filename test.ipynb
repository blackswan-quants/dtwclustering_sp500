{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Download S&P 500 data for the last 5 years\n",
    "df_sp500 = yf.download(\"^GSPC\", period=\"max\", interval=\"5d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class DTWClustering():\n",
    "    \"\"\"\n",
    "    A class for clustering using Euclidean and FastDTW distances with multiple methods (Hierarchical, KMeans, DBSCAN).\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input dataframe containing time-series data.\n",
    "    \n",
    "    Methods:\n",
    "        validate_dataframe(): Validates the input dataframe.\n",
    "        standardization(): Standardizes the inputs.\n",
    "        preprocessing(): Computes percentage change and prepares dataframes for clustering and performance analysis.\n",
    "        hierarchical_clustering(): Performs clustering using hierarchical methods (Euclidean or FastDTW distance).\n",
    "        kmeans_clustering(): Performs clustering using KMeans (Euclidean or FastDTW distance).\n",
    "        dbscan_clustering(): Performs clustering using DBSCAN (Euclidean or FastDTW distance).\n",
    "        plot_dendrogram(): Plots the dendrogram for hierarchical clustering results.\n",
    "        analyze_clusters(): Calculates silhouette scores and outputs analysis results.\n",
    "        elbow_method(): Calculates the optimal number of K for kmeans clustering by plotting the curve to find the elbow.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Initialize the DTWClustering object.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Input dataframe with time-series data.\n",
    "        \"\"\"\n",
    "        self.df_actual = df\n",
    "        self.df_pct_change = None\n",
    "        self.validate_dataframe()\n",
    "\n",
    "    def validate_dataframe(self):\n",
    "        \"\"\"\n",
    "        Validate the input dataframe to ensure no missing or infinite values.\n",
    "        \"\"\"\n",
    "        if not isinstance(self.df_actual, pd.DataFrame):\n",
    "            raise ValueError(\"Input must be a Pandas DataFrame.\")\n",
    "        if self.df_actual.isnull().any().any():\n",
    "            raise ValueError(\"Data contains missing values.\")\n",
    "        if not np.isfinite(self.df_actual.values).all():\n",
    "            raise ValueError(\"Data contains infinite values.\")\n",
    "        if not np.issubdtype(self.df_actual.dtypes.values[0], np.number):\n",
    "            raise ValueError(\"Data contains non-numeric values.\")\n",
    "            \n",
    "    def preprocessing(self):\n",
    "        \"\"\"\n",
    "        Transform the input dataframe by computing the percentage change for each value \n",
    "        relative to the previous row. This creates two dataframes:\n",
    "        - self.df_actual: The actual values.\n",
    "        - self.df_pct_change: The percentage change of the actual values.\n",
    "\n",
    "        Notes:\n",
    "            - The first row will contain NaN values after the percentage change.\n",
    "            - NaN values are backward filled to handle missing data.\n",
    "        \"\"\"\n",
    "        # Compute the percentage change\n",
    "        pct_change_df = self.df_actual.pct_change()\n",
    "\n",
    "        # Backward fill NaN values that result from the percentage change\n",
    "        self.df_pct_change = pct_change_df.bfill()\n",
    "\n",
    "        # Optional: Reset index to keep it clean (if needed)\n",
    "        self.df_pct_change.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    def compute_fastdtw_distances(self, df_to_use):\n",
    "        \"\"\"\n",
    "        Compute pairwise FastDTW distances for the dataset.\n",
    "\n",
    "        Args:\n",
    "            df_to_use (pd.DataFrame): The dataframe to use for distance calculation (either pct_change or actual values).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A condensed distance matrix for hierarchical clustering.\n",
    "        \"\"\"\n",
    "        # Get the number of tickers (columns in df_to_use)\n",
    "        n = df_to_use.shape[1]\n",
    "        distances = []\n",
    "\n",
    "        # Iterate over pairs of columns (tickers)\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                # Extract the time series for the two tickers (as 1D arrays)\n",
    "                x = df_to_use.iloc[:, i].values.squeeze()  # Column i\n",
    "                y = df_to_use.iloc[:, j].values.squeeze()  # Column j\n",
    "\n",
    "                # Compute the FastDTW distance with Euclidean distance (dist=2)\n",
    "                dist, _ = fastdtw(x, y, dist=2)\n",
    "                distances.append(dist)\n",
    "\n",
    "        return np.array(distances)\n",
    "\n",
    "    def hierarchical_clustering(self, k=None, distance_metric=\"euclidean\"):\n",
    "        \"\"\"\n",
    "        Perform hierarchical clustering on the percentage change data using a specified distance metric.\n",
    "\n",
    "        Args:\n",
    "            k (int, optional): The desired number of clusters. If not provided, \n",
    "                                the user must specify the value of k.\n",
    "            distance_metric (str, optional): The distance metric to use for clustering. \n",
    "                                            Can be one of 'euclidean' or 'fastdtw'. \n",
    "                                            Default is 'euclidean'.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the results of the hierarchical clustering. \n",
    "                The dictionary includes:\n",
    "                - 'linkage_matrix' (ndarray): The linkage matrix resulting from the hierarchical clustering.\n",
    "                - 'clusters' (ndarray): The cluster labels for each data point.\n",
    "                - 'silhouette' (float or str): The silhouette score of the clustering, or 'N/A' if the score cannot be computed (e.g., for non-Euclidean distance metrics).\n",
    "                - 'time' (float): The time elapsed during the clustering process, in seconds.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        data_for_clustering = self.df_pct_change.values.T\n",
    "\n",
    "\n",
    "        # Calculate pairwise distances\n",
    "        if distance_metric == \"euclidean\":\n",
    "            pairwise_distances = pdist(data_for_clustering, metric=\"euclidean\")  # Condensed matrix\n",
    "            method = \"ward\"\n",
    "        elif distance_metric == \"fastdtw\":\n",
    "            pairwise_distances = self.compute_fastdtw_distances(self.df_pct_change)\n",
    "            method = \"average\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported distance metric: {distance_metric}\")\n",
    "\n",
    "        # Perform hierarchical clustering\n",
    "        Z = linkage(pairwise_distances, method=method)\n",
    "\n",
    "        # Ensure k is provided for clustering\n",
    "        if k is None:\n",
    "            raise ValueError(\"Please provide a value for k (number of clusters).\")\n",
    "\n",
    "        # Assign cluster labels\n",
    "        clusters = fcluster(Z, k, criterion=\"maxclust\")\n",
    "\n",
    "        # Validate that at least 2 clusters are formed\n",
    "        unique_clusters = len(set(clusters))\n",
    "        if unique_clusters <= 1:\n",
    "            raise ValueError(f\"Only {unique_clusters} clusters were formed. Adjust k or check your data.\")\n",
    "\n",
    "        # Compute silhouette score only for Euclidean distances\n",
    "        if distance_metric == \"euclidean\":\n",
    "            silhouette = silhouette_score(self.df_pct_change.values.T, clusters, metric=\"euclidean\")\n",
    "        else:\n",
    "            silhouette = \"N/A\"\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        # Return clustering results\n",
    "        return {\n",
    "            \"linkage_matrix\": Z,\n",
    "            \"clusters\": clusters,\n",
    "            \"silhouette\": silhouette,\n",
    "            \"time\": elapsed_time\n",
    "        }\n",
    "\n",
    "    def kmeans_clustering(self, k, distance_metric=\"euclidean\"):\n",
    "        \"\"\"\n",
    "        Perform KMeans clustering using specified distance metric.\n",
    "\n",
    "        Args:\n",
    "            k (int): Number of clusters.\n",
    "            distance_metric (str): Distance metric ('euclidean' or 'fastdtw').\n",
    "\n",
    "        Returns:\n",
    "            dict: Clustering results, including cluster labels, silhouette score, and elapsed time.\n",
    "        \"\"\"\n",
    "        if k <= 0:\n",
    "            raise ValueError(\"k must be a positive integer.\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if distance_metric == \"euclidean\":\n",
    "        # Apply KMeans clustering on the percentage change data\n",
    "            model = KMeans(n_clusters=k)\n",
    "            clusters = model.fit_predict(self.df_pct_change.values.T)  # Use pct_change data as input\n",
    "        else:\n",
    "            raise ValueError(\"KMeans does not support pairwise distance matrices directly. Consider using another clustering algorithm.\")\n",
    "\n",
    "        # Compute silhouette score\n",
    "        if distance_metric == \"euclidean\":\n",
    "            silhouette = silhouette_score(self.df_pct_change.values.T, clusters, metric=\"euclidean\")\n",
    "        else:\n",
    "            silhouette = \"N/A\"  # Cannot compute silhouette score for non-Euclidean distances\n",
    "        elapsed_time = time.time() - start_time\n",
    "        return {\"clusters\": clusters, \"silhouette\": silhouette, \"time\": elapsed_time}\n",
    "\n",
    "    def dbscan_clustering(self, eps=0.5, min_samples=5, distance_metric=\"euclidean\"):\n",
    "        \"\"\"\n",
    "        Perform DBSCAN clustering using specified distance metric.\n",
    "\n",
    "        Args:\n",
    "            eps (float): Maximum distance between two samples to be considered as in the same neighborhood.\n",
    "            min_samples (int): Minimum number of samples in a neighborhood to form a cluster.\n",
    "            distance_metric (str): Distance metric ('euclidean' or 'fastdtw').\n",
    "\n",
    "        Returns:\n",
    "            dict: Clustering results, including cluster labels and elapsed time.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Compute pairwise distances\n",
    "        if distance_metric == \"euclidean\":\n",
    "            pairwise_distances = pdist(self.df_pct_change.values.T, metric=\"euclidean\")\n",
    "        elif distance_metric == \"fastdtw\":\n",
    "            pairwise_distances = self.compute_fastdtw_distances(self.df_pct_change)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported distance metric. Use 'euclidean' or 'fastdtw'.\")\n",
    "\n",
    "        # Convert pairwise_distances into a square matrix\n",
    "        pairwise_distances_square = squareform(pairwise_distances)\n",
    "\n",
    "        # Apply DBSCAN clustering with the precomputed distance matrix\n",
    "        model = DBSCAN(metric=\"precomputed\", eps=eps, min_samples=min_samples)\n",
    "        clusters = model.fit_predict(pairwise_distances_square)\n",
    "\n",
    "        # Calculate elapsed time\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        # Optionally, compute silhouette score, excluding noise points (-1)\n",
    "        if len(set(clusters)) > 1:  # Make sure there is more than one cluster (excluding noise)\n",
    "            valid_clusters = clusters != -1\n",
    "            if valid_clusters.any():  # Ensure there are valid points to calculate silhouette score\n",
    "                silhouette = silhouette_score(self.df_pct_change.values.T[valid_clusters, :], clusters[valid_clusters], metric=\"euclidean\")\n",
    "            else:\n",
    "                silhouette = \"N/A\"\n",
    "        else:\n",
    "            silhouette = \"N/A\"\n",
    "\n",
    "        return {\"clusters\": clusters, \"silhouette\": silhouette, \"time\": elapsed_time}\n",
    "    def plot_dendrogram(self, Z, title=\"Dendrogram\"):\n",
    "        \"\"\"\n",
    "        Plot a dendrogram from a linkage matrix.\n",
    "\n",
    "        Args:\n",
    "            Z (array): Linkage matrix.\n",
    "            title (str): Title for the plot.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        dendrogram(Z)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Cluster Size\")\n",
    "        plt.ylabel(\"Distance\")\n",
    "        plt.show()\n",
    "    def analyze_clusters(self, clustering_results, k, title_prefix=\"\"):\n",
    "        \"\"\"\n",
    "        Analyze clusters, calculate and display cluster-level statistics based on actual values.\n",
    "\n",
    "        Args:\n",
    "            clustering_results (dict): Output from clustering method.\n",
    "            k (int): Number of clusters.\n",
    "            title_prefix (str): Prefix for titles in outputs.\n",
    "        \"\"\"\n",
    "        clusters = clustering_results[\"clusters\"]\n",
    "        cluster_labels = np.unique(clusters)\n",
    "        stats = []\n",
    "\n",
    "        for label in cluster_labels:\n",
    "            idx = np.where(clusters == label)\n",
    "            cluster_data = self.df_actual.iloc[idx]\n",
    "            total_return = cluster_data.sum().sum()\n",
    "            average = cluster_data.mean().mean()\n",
    "            variance = cluster_data.var().mean()\n",
    "            stats.append({\"Cluster\": label, \"Total Return\": total_return, \"Average\": average, \"Variance\": variance})\n",
    "\n",
    "        stats_df = pd.DataFrame(stats)\n",
    "        print(f\"\\n{title_prefix} Cluster Analysis\")\n",
    "        print(stats_df)\n",
    "\n",
    "    def elbow_method(self, max_k=10):\n",
    "        \"\"\"\n",
    "        Plot the curve to identify the 'elbow' that corresponds to the optimal number of clusters (K).\n",
    "        \"\"\"\n",
    "        sse = []  # Sum of squared errors for each K\n",
    "        k_rng = range(1, max_k + 1)  # Test K values from 1 to max_k\n",
    "\n",
    "        # Compute SSE for each value of K\n",
    "        for k in k_rng:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42)  # Random state for reproducibility\n",
    "            kmeans.fit(self.df_pct_change.values)  # Fit KMeans to the pct_change data\n",
    "            sse.append(kmeans.inertia_)  # Append the inertia (SSE)\n",
    "\n",
    "        # Plot the SSE curve\n",
    "        plt.figure(figsize=(8, 6))  # Set figure size\n",
    "        plt.plot(k_rng, sse, linewidth=2, marker='o', linestyle='--', color='b')\n",
    "        plt.title(\"Elbow Method for Optimal K\")\n",
    "        plt.xlabel(\"Number of Clusters (K)\")\n",
    "        plt.ylabel(\"Sum of Squared Errors (SSE)\")\n",
    "        plt.grid(True)  # Add gridlines for better visibility\n",
    "        plt.xticks(k_rng)  # Ensure all K values appear on the x-axis\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giuli\\AppData\\Local\\Temp\\ipykernel_2084\\901571968.py:1: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_sp500.fillna(method='ffill', inplace=True)  # Forward-fill for existing gaps\n",
      "C:\\Users\\giuli\\AppData\\Local\\Temp\\ipykernel_2084\\901571968.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_sp500.fillna(method='bfill', inplace=True)  # Backward-fill for leading gaps\n"
     ]
    }
   ],
   "source": [
    "df_sp500.fillna(method='ffill', inplace=True)  # Forward-fill for existing gaps\n",
    "df_sp500.fillna(method='bfill', inplace=True)  # Backward-fill for leading gaps\n",
    "# Take the first 50 timestamps from df_sp500\n",
    "df_sp500_first_50 = df_sp500.iloc[:10]  # This selects the first 50 rows\n",
    "# Initialize the DTWClustering with the sliced DataFrame\n",
    "dtw_clustering = DTWClustering(df_sp500_first_50)\n",
    "\n",
    "dtw_clustering.preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>^GSPC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1927-12-30 00:00:00+00:00</th>\n",
       "      <td>17.660000</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928-01-04 00:00:00+00:00</th>\n",
       "      <td>17.719999</td>\n",
       "      <td>17.719999</td>\n",
       "      <td>17.719999</td>\n",
       "      <td>17.719999</td>\n",
       "      <td>17.719999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928-01-09 00:00:00+00:00</th>\n",
       "      <td>17.500000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928-01-19 00:00:00+00:00</th>\n",
       "      <td>17.379999</td>\n",
       "      <td>17.379999</td>\n",
       "      <td>17.379999</td>\n",
       "      <td>17.379999</td>\n",
       "      <td>17.379999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928-01-24 00:00:00+00:00</th>\n",
       "      <td>17.709999</td>\n",
       "      <td>17.709999</td>\n",
       "      <td>17.709999</td>\n",
       "      <td>17.709999</td>\n",
       "      <td>17.709999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928-02-03 00:00:00+00:00</th>\n",
       "      <td>17.400000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928-02-08 00:00:00+00:00</th>\n",
       "      <td>17.490000</td>\n",
       "      <td>17.490000</td>\n",
       "      <td>17.490000</td>\n",
       "      <td>17.490000</td>\n",
       "      <td>17.490000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928-02-23 00:00:00+00:00</th>\n",
       "      <td>17.129999</td>\n",
       "      <td>17.129999</td>\n",
       "      <td>17.129999</td>\n",
       "      <td>17.129999</td>\n",
       "      <td>17.129999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928-02-28 00:00:00+00:00</th>\n",
       "      <td>17.160000</td>\n",
       "      <td>17.160000</td>\n",
       "      <td>17.160000</td>\n",
       "      <td>17.160000</td>\n",
       "      <td>17.160000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928-03-09 00:00:00+00:00</th>\n",
       "      <td>17.930000</td>\n",
       "      <td>17.930000</td>\n",
       "      <td>17.930000</td>\n",
       "      <td>17.930000</td>\n",
       "      <td>17.930000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price                      Adj Close      Close       High        Low  \\\n",
       "Ticker                         ^GSPC      ^GSPC      ^GSPC      ^GSPC   \n",
       "Date                                                                    \n",
       "1927-12-30 00:00:00+00:00  17.660000  17.660000  17.660000  17.660000   \n",
       "1928-01-04 00:00:00+00:00  17.719999  17.719999  17.719999  17.719999   \n",
       "1928-01-09 00:00:00+00:00  17.500000  17.500000  17.500000  17.500000   \n",
       "1928-01-19 00:00:00+00:00  17.379999  17.379999  17.379999  17.379999   \n",
       "1928-01-24 00:00:00+00:00  17.709999  17.709999  17.709999  17.709999   \n",
       "1928-02-03 00:00:00+00:00  17.400000  17.400000  17.400000  17.400000   \n",
       "1928-02-08 00:00:00+00:00  17.490000  17.490000  17.490000  17.490000   \n",
       "1928-02-23 00:00:00+00:00  17.129999  17.129999  17.129999  17.129999   \n",
       "1928-02-28 00:00:00+00:00  17.160000  17.160000  17.160000  17.160000   \n",
       "1928-03-09 00:00:00+00:00  17.930000  17.930000  17.930000  17.930000   \n",
       "\n",
       "Price                           Open Volume  \n",
       "Ticker                         ^GSPC  ^GSPC  \n",
       "Date                                         \n",
       "1927-12-30 00:00:00+00:00  17.660000      0  \n",
       "1928-01-04 00:00:00+00:00  17.719999      0  \n",
       "1928-01-09 00:00:00+00:00  17.500000      0  \n",
       "1928-01-19 00:00:00+00:00  17.379999      0  \n",
       "1928-01-24 00:00:00+00:00  17.709999      0  \n",
       "1928-02-03 00:00:00+00:00  17.400000      0  \n",
       "1928-02-08 00:00:00+00:00  17.490000      0  \n",
       "1928-02-23 00:00:00+00:00  17.129999      0  \n",
       "1928-02-28 00:00:00+00:00  17.160000      0  \n",
       "1928-03-09 00:00:00+00:00  17.930000      0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sp500_first_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#elbow method for kmeans clustering\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdtw_clustering\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melbow_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 291\u001b[0m, in \u001b[0;36mDTWClustering.elbow_method\u001b[1;34m(self, max_k)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_rng:\n\u001b[0;32m    290\u001b[0m     kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mk, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)  \u001b[38;5;66;03m# Random state for reproducibility\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     \u001b[43mkmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf_pct_change\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Fit KMeans to the pct_change data\u001b[39;00m\n\u001b[0;32m    292\u001b[0m     sse\u001b[38;5;241m.\u001b[39mappend(kmeans\u001b[38;5;241m.\u001b[39minertia_)  \u001b[38;5;66;03m# Append the inertia (SSE)\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;66;03m# Plot the SSE curve\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giuli\\Repositories\\dtwclustering_sp500\\venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giuli\\Repositories\\dtwclustering_sp500\\venv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1464\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[0;32m   1439\u001b[0m \n\u001b[0;32m   1440\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1464\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1466\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1468\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1470\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1471\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1473\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m   1475\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n",
      "File \u001b[1;32mc:\\Users\\giuli\\Repositories\\dtwclustering_sp500\\venv\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\giuli\\Repositories\\dtwclustering_sp500\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giuli\\Repositories\\dtwclustering_sp500\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giuli\\Repositories\\dtwclustering_sp500\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "#elbow method for kmeans clustering\n",
    "dtw_clustering.elbow_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hierarchical Clustering with FastDTW Distance:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Hierarchical Clustering with FastDTW Distance\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHierarchical Clustering with FastDTW Distance:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m hierarchical_results_fastdtw \u001b[38;5;241m=\u001b[39m \u001b[43mdtw_clustering\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhierarchical_clustering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfastdtw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(hierarchical_results_fastdtw)\n",
      "Cell \u001b[1;32mIn[14], line 132\u001b[0m, in \u001b[0;36mDTWClustering.hierarchical_clustering\u001b[1;34m(self, k, distance_metric)\u001b[0m\n\u001b[0;32m    130\u001b[0m     method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m distance_metric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfastdtw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 132\u001b[0m     pairwise_distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_fastdtw_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf_pct_change\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[14], line 99\u001b[0m, in \u001b[0;36mDTWClustering.compute_fastdtw_distances\u001b[1;34m(self, df_to_use)\u001b[0m\n\u001b[0;32m     96\u001b[0m         y \u001b[38;5;241m=\u001b[39m df_to_use\u001b[38;5;241m.\u001b[39miloc[:, j]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39msqueeze()  \u001b[38;5;66;03m# Column j\u001b[39;00m\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;66;03m# Compute the FastDTW distance with Euclidean distance (dist=2)\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m         dist, _ \u001b[38;5;241m=\u001b[39m \u001b[43mfastdtw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m         distances\u001b[38;5;241m.\u001b[39mappend(dist)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(distances)\n",
      "File \u001b[1;32mc:\\Users\\giuli\\Repositories\\dtwclustering_sp500\\venv\\Lib\\site-packages\\fastdtw\\fastdtw.py:53\u001b[0m, in \u001b[0;36mfastdtw\u001b[1;34m(x, y, radius, dist)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m''' return the approximate distance between 2 time series with O(N)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    time and memory complexity\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    (2.0, [(0, 0), (1, 0), (2, 1), (3, 2), (4, 2)])\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     52\u001b[0m x, y, dist \u001b[38;5;241m=\u001b[39m __prep_inputs(x, y, dist)\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__fastdtw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giuli\\Repositories\\dtwclustering_sp500\\venv\\Lib\\site-packages\\fastdtw\\fastdtw.py:73\u001b[0m, in \u001b[0;36m__fastdtw\u001b[1;34m(x, y, radius, dist)\u001b[0m\n\u001b[0;32m     70\u001b[0m x_shrinked \u001b[38;5;241m=\u001b[39m __reduce_by_half(x)\n\u001b[0;32m     71\u001b[0m y_shrinked \u001b[38;5;241m=\u001b[39m __reduce_by_half(y)\n\u001b[0;32m     72\u001b[0m distance, path \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m---> 73\u001b[0m     \u001b[43m__fastdtw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_shrinked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_shrinked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m window \u001b[38;5;241m=\u001b[39m __expand_window(path, \u001b[38;5;28mlen\u001b[39m(x), \u001b[38;5;28mlen\u001b[39m(y), radius)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m __dtw(x, y, window, dist\u001b[38;5;241m=\u001b[39mdist)\n",
      "File \u001b[1;32mc:\\Users\\giuli\\Repositories\\dtwclustering_sp500\\venv\\Lib\\site-packages\\fastdtw\\fastdtw.py:73\u001b[0m, in \u001b[0;36m__fastdtw\u001b[1;34m(x, y, radius, dist)\u001b[0m\n\u001b[0;32m     70\u001b[0m x_shrinked \u001b[38;5;241m=\u001b[39m __reduce_by_half(x)\n\u001b[0;32m     71\u001b[0m y_shrinked \u001b[38;5;241m=\u001b[39m __reduce_by_half(y)\n\u001b[0;32m     72\u001b[0m distance, path \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m---> 73\u001b[0m     \u001b[43m__fastdtw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_shrinked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_shrinked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m window \u001b[38;5;241m=\u001b[39m __expand_window(path, \u001b[38;5;28mlen\u001b[39m(x), \u001b[38;5;28mlen\u001b[39m(y), radius)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m __dtw(x, y, window, dist\u001b[38;5;241m=\u001b[39mdist)\n",
      "File \u001b[1;32mc:\\Users\\giuli\\Repositories\\dtwclustering_sp500\\venv\\Lib\\site-packages\\fastdtw\\fastdtw.py:68\u001b[0m, in \u001b[0;36m__fastdtw\u001b[1;34m(x, y, radius, dist)\u001b[0m\n\u001b[0;32m     65\u001b[0m min_time_size \u001b[38;5;241m=\u001b[39m radius \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m<\u001b[39m min_time_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y) \u001b[38;5;241m<\u001b[39m min_time_size:\n\u001b[1;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdtw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m x_shrinked \u001b[38;5;241m=\u001b[39m __reduce_by_half(x)\n\u001b[0;32m     71\u001b[0m y_shrinked \u001b[38;5;241m=\u001b[39m __reduce_by_half(y)\n",
      "File \u001b[1;32mc:\\Users\\giuli\\Repositories\\dtwclustering_sp500\\venv\\Lib\\site-packages\\fastdtw\\fastdtw.py:130\u001b[0m, in \u001b[0;36mdtw\u001b[1;34m(x, y, dist)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m''' return the distance between 2 time series without approximation\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    (2.0, [(0, 0), (1, 0), (2, 1), (3, 2), (4, 2)])\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    129\u001b[0m x, y, dist \u001b[38;5;241m=\u001b[39m __prep_inputs(x, y, dist)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__dtw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giuli\\Repositories\\dtwclustering_sp500\\venv\\Lib\\site-packages\\fastdtw\\fastdtw.py:148\u001b[0m, in \u001b[0;36m__dtw\u001b[1;34m(x, y, window, dist)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (i \u001b[38;5;241m==\u001b[39m j \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    147\u001b[0m     path\u001b[38;5;241m.\u001b[39mappend((i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, j\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 148\u001b[0m     i, j \u001b[38;5;241m=\u001b[39m \u001b[43mD\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, D[i, j][\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    149\u001b[0m path\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (D[len_x, len_y][\u001b[38;5;241m0\u001b[39m], path)\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Hierarchical Clustering with FastDTW Distance\n",
    "print(\"\\nHierarchical Clustering with FastDTW Distance:\")\n",
    "hierarchical_results_fastdtw = dtw_clustering.hierarchical_clustering(k=2, distance_metric=\"fastdtw\")\n",
    "print(hierarchical_results_fastdtw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Clustering with Euclidean Distance\n",
    "print(\"\\nHierarchical Clustering with Euclidean Distance:\")\n",
    "hierarchical_results_euclidean = dtw_clustering.hierarchical_clustering(k=4, distance_metric=\"euclidean\")\n",
    "print(hierarchical_results_euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans Clustering with Euclidean Distance\n",
    "print(\"\\nKMeans Clustering with Euclidean Distance:\")\n",
    "kmeans_results_euclidean = dtw_clustering.kmeans_clustering(k=4, distance_metric=\"euclidean\")\n",
    "print(kmeans_results_euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_cluster_fastDTW=hierarchical_results_fastdtw[\"clusters\"] -1\n",
    "\n",
    "hierarchical_cluster_euclidean=hierarchical_results_euclidean[\"clusters\"] -1\n",
    "\n",
    "dtw_clustering_transposed_actual_values=dtw_clustering.df_actual.T\n",
    "\n",
    "kmeans_cluster_euclidean=kmeans_results_euclidean[\"clusters\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_clustering_transposed_actual_values['Hierarchical_Cluster_euclidean'] = hierarchical_cluster_euclidean\n",
    "dtw_clustering_transposed_actual_values['Hierarchical_Cluster_DTW'] = hierarchical_cluster_fastDTW\n",
    "dtw_clustering_transposed_actual_values['KMeans_Cluster'] = kmeans_cluster_euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_clustering_transposed_pct_change=dtw_clustering.df_pct_change.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_clustering_transposed_pct_change['Hierarchical_Cluster_euclidean'] = hierarchical_cluster_euclidean\n",
    "dtw_clustering_transposed_pct_change['Hierarchical_Cluster_DTW'] = hierarchical_cluster_fastDTW\n",
    "dtw_clustering_transposed_pct_change['KMeans_Cluster'] = kmeans_cluster_euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dendrograms\n",
    "print(\"\\nPlotting Dendrogram for Euclidean Hierarchical Clustering:\")\n",
    "dtw_clustering.plot_dendrogram(hierarchical_results_euclidean[\"linkage_matrix\"], title=\"Euclidean Distance Dendrogram\")\n",
    "\n",
    "print(\"\\nPlotting Dendrogram for FastDTW Hierarchical Clustering:\")\n",
    "dtw_clustering.plot_dendrogram(hierarchical_results_fastdtw[\"linkage_matrix\"], title=\"FastDTW Distance Dendrogram\")\n",
    "\n",
    "# Analyze clusters for both hierarchical methods\n",
    "print(\"\\nCluster Analysis for Euclidean Hierarchical Clustering:\")\n",
    "dtw_clustering.analyze_clusters(hierarchical_results_euclidean, k=3, title_prefix=\"Euclidean Hierarchical\")\n",
    "\n",
    "print(\"\\nCluster Analysis for FastDTW Hierarchical Clustering:\")\n",
    "dtw_clustering.analyze_clusters(hierarchical_results_fastdtw, k=4, title_prefix=\"FastDTW Hierarchical\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
